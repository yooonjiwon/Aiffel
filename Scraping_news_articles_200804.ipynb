{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Scraping News Articles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Library\n",
    "import pandas as pd #df\n",
    "import matplotlib.font_manager as fm # font\n",
    "import matplotlib.pyplot as plt # font\n",
    "import requests # for scraping\n",
    "from bs4 import BeautifulSoup # for scraping\n",
    "from newspaper import Article\n",
    "from konlpy.tag import Mecab, Hannanum, Kkma, Komoran, Okt # 형태소\n",
    "# machine learning\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn import metrics\n",
    "from sklearn.feature_extraction.text import HashingVectorizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 데이터 import"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### naver news article 직접 수집"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### URL 수집하는 함수\n",
    "- 원하는 페이지수(page_num), 카테고리 번호(code), 날짜(date) 입력   \n",
    "-예를 들어 2020년 8월 3일 경제기사(코드 101)를 2페이지까지 받고 싶으면, make_urllist(2,101,20200803)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_urllist(page_num,code,date):\n",
    "    urllist=[]\n",
    "    for i in range(1,page_num+1):\n",
    "        url='https://news.naver.com/main/list.nhn?mode=LSD&mid=sec&sid1='+str(code)+'&date='+str(date)+'&page='+str(i)   \n",
    "        news = requests.get(url)\n",
    "        \n",
    "        # BeautifulSoup의 인스턴스 생성. \n",
    "        soup = BeautifulSoup(news.content,'html.parser')\n",
    "        \n",
    "        # CASE 1\n",
    "        news_list = soup.select('.newsflash_body .type06_headline li dl')\n",
    "        # CASE 2\n",
    "        news_list.extend(soup.select('.newsflash_body .type06 li dl'))\n",
    "        \n",
    "    # 각 뉴스로부터 a 태그인 <a href ='주소'> 에서 '주소'만을 가져옵니다.\n",
    "        for line in news_list:\n",
    "            urllist.append(line.a.get('href'))\n",
    "    return urllist       "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 받아온 url이용하여 기사 저장\n",
    "- url list와 분류를 이용하여 기사 내용 저장\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 뉴스기사 분류 코드\n",
    "idx2word = {'101' : '경제', '102' : '사회', '103' : '생활/문화', '105' : 'IT/과학'}\n",
    "\n",
    "#- 데이터프레임을 생성하는 함수입니다.\n",
    "def make_data(urllist, code):\n",
    "    text_list = []\n",
    "    for url in urllist:\n",
    "        article = Article(url, language='ko')\n",
    "        article.download()\n",
    "        article.parse()\n",
    "        text_list.append(article.text)\n",
    "    \n",
    "    #- 데이터프레임의 'news' 키 아래 파싱한 텍스트를 밸류로 붙여줍니다.\n",
    "    df = pd.DataFrame({'news': text_list})\n",
    "\n",
    "    #- 데이터프레임의 'code' 키 아래 한글 카테고리명을 붙여줍니다.\n",
    "    df['code'] = idx2word[str(code)]\n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 여러 카테고리의 기사 가져오기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 페이지 수, code_list, 날짜 기준으로 수집하는 함수\n",
    "def make_total_data(page_num, code_list, date):\n",
    "    df = None\n",
    "    \n",
    "    for code in code_list:\n",
    "        url_list = make_urllist(page_num, code, date)\n",
    "        df_temp = make_data(url_list, code)\n",
    "        print(str(code)+'번 코드에 대한 데이터를 만들었습니다.')\n",
    "        \n",
    "    if df is not None:\n",
    "        df = pd.concat([df, df_temp])\n",
    "    else:\n",
    "        df = df_temp\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df = make_total_data(100, code_list, 20200803)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 미리 수집된 뉴스 기사 데이터 사용"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "df =pd.read_csv('/home/aiffel0036/aiffel/data_represent/data/news_data.csv')\n",
    "df2 =pd.read_csv('/home/aiffel0036/aiffel/data_represent/data/news_data2.csv')\n",
    "\n",
    "df = pd.concat([df2,df], axis=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 데이터 탐색 및 정제"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# head, null, length 확인하는 function\n",
    "def data_exp(df):\n",
    "    print(\"head출력: \\n\", df.head())\n",
    "    print(\"null값 확인: \\n\", df.isnull().sum())\n",
    "    print(\"length: \\n\", len(df))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "head출력: \n",
      "                                                 news code\n",
      "0  기사 섹션 분류 안내\\n\\n기사의 섹션 정보는 해당 언론사의 분류를 따르고 있습니다...   경제\n",
      "1  ▶제21대 총선 실시간 개표 현황 및 결과 보기\\n\\n총선에서 여당이 다시 한 번 ...   경제\n",
      "2  [뉴욕=AP/뉴시스]지난 10일 뉴욕 증권거래소 건물에 미국 국기가 게양되어 있다....   경제\n",
      "3  부산지역 주유소에서 판매하는 기름값이 휘발유는 평균 1200원대, 경유는 1000원...   경제\n",
      "4  담배업계가 소비자의 취향을 저격한 다양한 담배 신제품들을 잇달아 선보이고 있다.이전...   경제\n",
      "null값 확인: \n",
      " news    0\n",
      "code    0\n",
      "dtype: int64\n",
      "length: \n",
      " 8827\n"
     ]
    }
   ],
   "source": [
    "data_exp(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 중복값 확인 및 제거"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "length: \n",
      " 6127\n"
     ]
    }
   ],
   "source": [
    "# 중복 값 확인 => 중복으로 수집되고 있기 때문\n",
    "df[df.duplicated(subset=['news'], keep = 'first')].sort_values(by=['news'])\n",
    "#df2[df2.duplicated(subset=['news'], keep = 'first')].sort_values(by=['news'])\n",
    "# 중복된 샘플들을 제거\n",
    "df.drop_duplicates(subset=['news'], inplace = True)\n",
    "#df2.drop_duplicates(subset=['news'], inplace = True)\n",
    "print(\"length: \\n\", len(df))\n",
    "df.head()\n",
    "# 확인해보니, 0번째는 기사가 아님 -> 삭제\n",
    "df = df[1:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 한글 외의 문자는 제거"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1       제대 총선 실시간 개표 현황 및 결과 보기총선에서 여당이 다시 한 번 승리를 거두면...\n",
       "2       뉴욕뉴시스지난 일 뉴욕 증권거래소 건물에 미국 국기가 게양되어 있다 신종 코로나바이...\n",
       "3       부산지역 주유소에서 판매하는 기름값이 휘발유는 평균 원대 경유는 원대에 돌입했다일 ...\n",
       "4       담배업계가 소비자의 취향을 저격한 다양한 담배 신제품들을 잇달아 선보이고 있다이전까...\n",
       "6       이데일리 박태진 기자 다음은 일자 이데일리신문 주요 기사다면민주 연승 자만 말고 협...\n",
       "                              ...                        \n",
       "4825    신종 코로나바이러스 감염증코로나 사태 이후 가정의 달 월에도 언택트비대면 신풍속도가...\n",
       "4826    는 소비자로부터 월 이용료 만만원을 받고 초고속 인터넷을 제공한다 그런 브로드밴드가...\n",
       "4827    머리를 긁고 있는 오랑우탄 몸을 긁는 행동을 따라 하는 것은 부정적 감정과 관련이 ...\n",
       "4828    가 오는 일 정식 출시하는 스마트폰 벨벳이 사실상 공짜폰이 될 전망이다 단말기 가격...\n",
       "4829    이미지제공게티이미지뱅크 이미지제공게티이미지뱅크  전자신문  전자신문인터넷 무단전재 ...\n",
       "Name: news, Length: 6126, dtype: object"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 정규 표현식을 이용해서 한글 외의 문자는 전부 제거합니다.\n",
    "df['news'] = df['news'].str.replace(\"[^ㄱ-ㅎㅏ-ㅣ가-힣 ]\",\"\")\n",
    "#df2['news'] = df2['news'].str.replace(\"[^ㄱ-ㅎㅏ-ㅣ가-힣 ]\",\"\")\n",
    "df['news']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 카테고리별 분포 확인"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "사회       2221\n",
      "생활/문화    1866\n",
      "IT/과학    1138\n",
      "경제        901\n",
      "Name: code, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "print(df['code'].value_counts())\n",
    "#print(df2['code'].value_counts())\n",
    "# 경제 > 사회 > 생활/문화 > IT/과학"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tokenization\n",
    "- 문자열을 특정 단위(token)으로 나누어 분석을 하게 됨.\n",
    "- 한국어의 경우 형태소 분석기를 이용하여 토큰화를 함.\n",
    "\n",
    "#### Eliminating stopwards\n",
    "- 자연어 처리에 불필요한 단어들. 한국어의 경우 조사, 접사가 이에 해당.\n",
    "- 미리 한번에 정의하기보다는 tokenization을 통해 여러 차례 확인하며 제거"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1203"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 조사 어미 파일 import\n",
    "josa = pd.read_csv('/home/aiffel0036/aiffel/data_represent/data/JOSA2.TXT', sep=\" \", header=None)\n",
    "eomi = pd.read_csv('/home/aiffel0036/aiffel/data_represent/data/EOMI2.TXT', sep=\" \", header=None)\n",
    "# to list\n",
    "josa = josa[0].values.tolist()\n",
    "eomi = eomi[0].values.tolist()\n",
    "# merge\n",
    "stopwords = josa + eomi + ['에','는','은','을','했','에게','있','이','의','하','한','다','과','때문','할','수','무단','따른','및','금지','전재','경향신문','기자','는데','가','등','들','파이낸셜','저작','등','뉴스','동영상']\n",
    "len(stopwords)\n",
    "#stopwords = ['에','는','은','을','했','에게','있','이','의','하','한','다','과','때문','할','수','무단','따른','및','금지','전재','경향신문','기자','는데','가','등','들','파이낸셜','저작','등','뉴스']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 토큰화 및 토큰화 과정에서 불용어를 제거하는 함수입니다.\n",
    "def preprocessing(data,tokenizer):\n",
    "    text_data = []\n",
    "    \n",
    "    for sentence in data:\n",
    "        temp_data = []\n",
    "        #- 토큰화\n",
    "        temp_data = tokenizer.morphs(sentence) \n",
    "        #- 불용어 제거\n",
    "        temp_data = [word for word in temp_data if not word in stopwords] \n",
    "        text_data.append(temp_data)\n",
    "    \n",
    "    text_data = list(map(' '.join, text_data))\n",
    "\n",
    "    return text_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "token_Mecab = Mecab()\n",
    "token_Hannanum = Hannanum()\n",
    "token_Kkma = Kkma()\n",
    "token_Komoran = Komoran()\n",
    "token_Okt = Okt()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 5.64 s, sys: 40.1 ms, total: 5.68 s\n",
      "Wall time: 5.68 s\n"
     ]
    }
   ],
   "source": [
    "%time text_Mecab = preprocessing(df['news'], token_Mecab)\n",
    "#text_Mecab2 = preprocessing(df2['news'], token_Mecab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "%time text_Hannanum = preprocessing(df['news'], token_Hannanum)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 다시 실행시키기 무서움 => 오래걸림\n",
    "text_Kkma = preprocessing(df['news'], token_Kkma)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 1min 43s, sys: 222 ms, total: 1min 43s\n",
      "Wall time: 1min 30s\n"
     ]
    }
   ],
   "source": [
    "%time text_Komoran = preprocessing(df['news'], token_Komoran)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 40min 29s, sys: 986 ms, total: 40min 30s\n",
      "Wall time: 5min 57s\n"
     ]
    }
   ],
   "source": [
    "%time text_Okt = preprocessing(df['news'], token_Okt)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model \n",
    "\n",
    "### Splitting data into train and test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "훈련용 뉴스 기사의 개수 : 4900\n",
      "테스트용 뉴스 기사의 개수 :  1226\n",
      "훈련용 레이블의 개수 :  4900\n",
      "테스트용 레이블의 개수 :  1226\n"
     ]
    }
   ],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(text_Mecab, df['code'],test_size=0.2, shuffle=True, random_state = 34)\n",
    "print('훈련용 뉴스 기사의 개수 :', len(X_train))\n",
    "print('테스트용 뉴스 기사의 개수 : ', len(X_test))\n",
    "print('훈련용 레이블의 개수 : ', len(y_train))\n",
    "print('테스트용 레이블의 개수 : ', len(y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "훈련용 뉴스 기사의 개수 : 4900\n",
      "테스트용 뉴스 기사의 개수 :  1226\n",
      "훈련용 레이블의 개수 :  4900\n",
      "테스트용 레이블의 개수 :  1226\n"
     ]
    }
   ],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(text_Hannanum, df['code'],test_size=0.2, shuffle=True, random_state = 34)\n",
    "print('훈련용 뉴스 기사의 개수 :', len(X_train))\n",
    "print('테스트용 뉴스 기사의 개수 : ', len(X_test))\n",
    "print('훈련용 레이블의 개수 : ', len(y_train))\n",
    "print('테스트용 레이블의 개수 : ', len(y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "훈련용 뉴스 기사의 개수 : 4900\n",
      "테스트용 뉴스 기사의 개수 :  1226\n",
      "훈련용 레이블의 개수 :  4900\n",
      "테스트용 레이블의 개수 :  1226\n"
     ]
    }
   ],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(text_Kkma, df['code'],test_size=0.2, shuffle=True, random_state = 34)\n",
    "print('훈련용 뉴스 기사의 개수 :', len(X_train))\n",
    "print('테스트용 뉴스 기사의 개수 : ', len(X_test))\n",
    "print('훈련용 레이블의 개수 : ', len(y_train))\n",
    "print('테스트용 레이블의 개수 : ', len(y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "훈련용 뉴스 기사의 개수 : 4900\n",
      "테스트용 뉴스 기사의 개수 :  1226\n",
      "훈련용 레이블의 개수 :  4900\n",
      "테스트용 레이블의 개수 :  1226\n"
     ]
    }
   ],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(text_Komoran, df['code'],test_size=0.2, shuffle=True, random_state = 34)\n",
    "print('훈련용 뉴스 기사의 개수 :', len(X_train))\n",
    "print('테스트용 뉴스 기사의 개수 : ', len(X_test))\n",
    "print('훈련용 레이블의 개수 : ', len(y_train))\n",
    "print('테스트용 레이블의 개수 : ', len(y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "훈련용 뉴스 기사의 개수 : 4900\n",
      "테스트용 뉴스 기사의 개수 :  1226\n",
      "훈련용 레이블의 개수 :  4900\n",
      "테스트용 레이블의 개수 :  1226\n"
     ]
    }
   ],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(text_Okt, df['code'],test_size=0.2, shuffle=True, random_state = 34)\n",
    "print('훈련용 뉴스 기사의 개수 :', len(X_train))\n",
    "print('테스트용 뉴스 기사의 개수 : ', len(X_test))\n",
    "print('훈련용 레이블의 개수 : ', len(y_train))\n",
    "print('테스트용 레이블의 개수 : ', len(y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TF-IDF vector로 변환\n",
    "- Term Frequency * Inverse Document Frequency의 약자\n",
    "- 단어가 많이 등장할 수록 관련이 많을 것이다라고 생각.\n",
    "- 하지만, 연관성 없이 여러번 나타날 때 문제가 됨.\n",
    "- 따라서 패널티 주기위해 IDF 사용. Log (Total number of Docs / number of Docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 텍스트를 입력하면 자동으로 TF-IDF vector로 바꾸어주는 전처리 함수\n",
    "def tfidf_vectorizer(data):\n",
    "    data_counts = count_vect.transform(data)\n",
    "    data_tfidf = tfidf_transformer.transform(data_counts)\n",
    "    return data_tfidf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 텍스트 데이터 벡터로 변환해야 함.\n",
    "# 단어의 수를 카운트하는 사이킷런의 카운트벡터라이저.\n",
    "count_vect = CountVectorizer()\n",
    "#print(count_vect.vocabulary_)\n",
    "# document-form matrix로 변환\n",
    "X_train_counts = count_vect.fit_transform(X_train)\n",
    "\n",
    "#- 카운트벡터라이저의 결과로부터 TF-IDF 결과를 얻습니다.\n",
    "tfidf_transformer = TfidfTransformer()\n",
    "X_train_tfidf = tfidf_transformer.fit_transform(X_train_counts)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 나이브 베이즈 분류기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "#- 나이브 베이즈 분류기를 수행합니다.\n",
    "#- X_train은 TF-IDF 벡터, y_train은 레이블입니다.\n",
    "clf = MultinomialNB().fit(X_train_tfidf, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "       IT/과학       0.84      0.73      0.78       224\n",
      "          경제       0.91      0.47      0.62       171\n",
      "          사회       0.73      0.94      0.82       446\n",
      "       생활/문화       0.79      0.76      0.77       385\n",
      "\n",
      "    accuracy                           0.78      1226\n",
      "   macro avg       0.82      0.72      0.75      1226\n",
      "weighted avg       0.79      0.78      0.77      1226\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# mecab\n",
    "y_pred = clf.predict(tfidf_vectorizer(X_test))\n",
    "print(metrics.classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "       IT/과학       0.87      0.68      0.76       224\n",
      "          경제       0.89      0.47      0.62       171\n",
      "          사회       0.73      0.93      0.82       446\n",
      "       생활/문화       0.76      0.77      0.77       385\n",
      "\n",
      "    accuracy                           0.77      1226\n",
      "   macro avg       0.81      0.71      0.74      1226\n",
      "weighted avg       0.79      0.77      0.76      1226\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# hannanum\n",
    "y_pred = clf.predict(tfidf_vectorizer(X_test))\n",
    "print(metrics.classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "       IT/과학       0.85      0.72      0.78       224\n",
      "          경제       0.87      0.55      0.67       171\n",
      "          사회       0.74      0.94      0.82       446\n",
      "       생활/문화       0.82      0.76      0.79       385\n",
      "\n",
      "    accuracy                           0.79      1226\n",
      "   macro avg       0.82      0.74      0.77      1226\n",
      "weighted avg       0.80      0.79      0.78      1226\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Kkma\n",
    "y_pred = clf.predict(tfidf_vectorizer(X_test))\n",
    "print(metrics.classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "       IT/과학       0.87      0.73      0.79       224\n",
      "          경제       0.86      0.54      0.66       171\n",
      "          사회       0.75      0.94      0.83       446\n",
      "       생활/문화       0.81      0.78      0.80       385\n",
      "\n",
      "    accuracy                           0.80      1226\n",
      "   macro avg       0.82      0.75      0.77      1226\n",
      "weighted avg       0.81      0.80      0.79      1226\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Komoran\n",
    "y_pred = clf.predict(tfidf_vectorizer(X_test))\n",
    "print(metrics.classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "       IT/과학       0.87      0.75      0.81       224\n",
      "          경제       0.87      0.48      0.62       171\n",
      "          사회       0.73      0.93      0.82       446\n",
      "       생활/문화       0.82      0.78      0.80       385\n",
      "\n",
      "    accuracy                           0.79      1226\n",
      "   macro avg       0.82      0.74      0.76      1226\n",
      "weighted avg       0.80      0.79      0.78      1226\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Okt\n",
    "y_pred = clf.predict(tfidf_vectorizer(X_test))\n",
    "print(metrics.classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "1. Kkma는 너무 오래걸려서 효율성이 떨어지기 때문에 사용하지 않는 것이 좋을 것 같다.\n",
    "2. f1-score는 Komoran이 가장 높았다.\n",
    "3. 데이터 전처리에 더 공을 들여야 할 것 같다 -> 아쉽게도 시간이 부족.   \n",
    "특히, 단어 frequency나 불용어를 더 자세하게 보아야 할 것 같다. 단어들을 하나씩 보면 이상하게 끊긴 것들이 은근 많음.   \n",
    "\n",
    "- 개선책: f1-score는 다들 비슷하므로 가장 속도가 빠른 Mecab을 이용하고, 데이터 전처리 단계에 더 공을 들일 것"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "aiffel",
   "language": "python",
   "name": "aiffel"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": true
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "position": {
    "height": "488px",
    "left": "1556px",
    "right": "20px",
    "top": "112px",
    "width": "350px"
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
